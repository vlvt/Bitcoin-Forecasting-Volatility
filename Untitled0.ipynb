{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuG46vECnJiYDEFGluByzO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlvt/Bitcoin-Forecasting-Volatility/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diabetes Prediction Project\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "## 2. Data Loading\n",
        "\n",
        "\n",
        "## 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "\n",
        "## 4. Data Preprocessing\n",
        "\n",
        "\n",
        "## 5. Modeling\n",
        "\n",
        "\n",
        "## 6. Model Evaluation\n",
        "\n",
        "\n",
        "## 7. Conclusions and Recommendations\n",
        "\n"
      ],
      "metadata": {
        "id": "IoEJdPkqBkSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Introduction\n",
        "\n",
        "- Briefly describe the problem and its importance.\n",
        "- Explain why you chose this dataset and project.\n"
      ],
      "metadata": {
        "id": "GvmScpN8CJ8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "df-GDm1ZCVuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yD-B_1bOCYLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Overview"
      ],
      "metadata": {
        "id": "JTQ6p4DNC0pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "9CtCJARRC3z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMissing. values per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "Sdf6Kb8LC9ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found 1 missing value in the target variable 'diabetes'.  \n",
        "Since the target variable is critical for supervised learning, we will drop this row from the dataset.\n"
      ],
      "metadata": {
        "id": "qNtiMWL1DUjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['diabetes'])\n"
      ],
      "metadata": {
        "id": "F9BwEv0_DXBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have completed the basic data overview:\n",
        "- All columns except the target variable have no missing values.\n",
        "- The target variable has one missing value, which we decided to drop.\n",
        "- The dataset is now ready for exploratory data analysis.\n"
      ],
      "metadata": {
        "id": "A2-QXFx6DitQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EDA"
      ],
      "metadata": {
        "id": "jEqFiywMDnzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this section, I will:\n",
        "- Visualize the distribution of key features.\n",
        "- Explore the relationships between features and the target variable.\n",
        "- Check for class imbalance in the target variable.\n",
        "- Analyze feature correlations.\n"
      ],
      "metadata": {
        "id": "fknvbNsKD10E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Target Variable Distribution\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.countplot(x = 'diabetes', data = df)\n",
        "plt.title('Distribution of Diabetes')\n",
        "plt.xlabel('Diabetes')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(df['diabetes'].value_counts())"
      ],
      "metadata": {
        "id": "cXKRMr5nDykt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Observation: Target Variable Imbalance\n",
        "\n",
        "- The dataset shows a significant imbalance between non-diabetic (0) and diabetic (1) cases:\n",
        "    - Non-diabetic: 75,549 cases (~91.5%)\n",
        "    - Diabetic: 7,015 cases (~8.5%)\n",
        "- This imbalance could affect model performance by biasing predictions toward the majority class.\n",
        "- To address this, we will consider:\n",
        "    - Using appropriate evaluation metrics (recall, precision, F1-score, ROC-AUC).\n",
        "    - Potentially applying resampling techniques (e.g., oversampling, undersampling) during model training.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "apbDPqYeGcC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Distributions\n",
        "\n",
        "numeric_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "\n",
        "for feature in numeric_features:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(df[feature], kde=True)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kioqjAfeEV1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        ">  Observations from Feature Distributions\n",
        "\n",
        "- **Age** shows a relatively uniform distribution from 0 to 80 years, with a noticeable spike at 80. This might indicate grouped values for older patients.\n",
        "- **BMI** has a suspiciously high peak around 40, suggesting potential data duplication or outliers. Further investigation is needed.\n",
        "- **HbA1c_level** and **blood_glucose_level** both exhibit stepped distributions with repeated identical values. This could result from rounding or discretization in data collection.\n",
        "- **Blood_glucose_level** also shows some high-value outliers (above 250), which might require outlier handling or transformation.\n",
        "\n"
      ],
      "metadata": {
        "id": "dmEOcUDjG2DO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplots by Target Variable\n",
        "\n",
        "for feature in numeric_features:\n",
        "  plt.figure(figsize=(6,4))\n",
        "  sns.boxplot(x = 'diabetes', y = feature, data = df)\n",
        "  plt.title(f'{feature} by Diabetes')\n",
        "  plt.xlabel('Diabetes')\n",
        "  plt.ylabel(feature)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "uFE5BOeQFIR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">  Observations from Boxplots by Diabetes\n",
        "\n",
        "- **Age:** Diabetic patients tend to be older than non-diabetic patients, aligning with medical expectations.\n",
        "- **BMI:** No significant difference in BMI between diabetic and non-diabetic patients, though many outliers are present.\n",
        "- **HbA1c_level:** Diabetic patients have clearly higher HbA1c levels compared to non-diabetic patients, indicating it is a strong predictor.\n",
        "- **Blood_glucose_level:** Diabetic patients also exhibit higher blood glucose levels, making this another important predictor.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gEHsr2SuHgaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation Heatmap\n",
        "\n",
        "plt.figure(figsize = (12,8))\n",
        "corr = df[numeric_features + ['diabetes']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap = 'coolwarm')\n",
        "plt.title(\"Feature Correlation Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zty6O0X5FkoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Analysis of Categorical Features\n",
        "\n"
      ],
      "metadata": {
        "id": "VmNwWcJJIUMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the correlation heatmap only includes numerical features, we will include all features in our modeling phase.  \n",
        "Categorical features like gender, hypertension, and smoking_history may have important relationships with diabetes that are not captured by simple correlation analysis.\n"
      ],
      "metadata": {
        "id": "nf7HH2GuIxid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = ['gender', 'hypertension', 'heart_disease', 'smoking_history']\n",
        "\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x=feature, hue='diabetes', data=df)\n",
        "    plt.title(f'{feature} by Diabetes')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "wmvrMySOIlp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Observations from Categorical Features\n",
        "\n",
        "- **Gender:** Both male and female groups have diabetic and non-diabetic patients. Gender alone may not be a strong predictor, but it could still provide additional context.\n",
        "- **Hypertension:** Patients with hypertension show a higher proportion of diabetes cases compared to those without hypertension.\n",
        "- **Heart Disease:** Patients with heart disease also exhibit a higher proportion of diabetes cases, suggesting potential predictive value.\n",
        "- **Smoking History:** All categories show diabetic cases, but categories like \"never\" and \"No Info\" dominate in count. This feature may benefit from one-hot encoding or category grouping during preprocessing.\n"
      ],
      "metadata": {
        "id": "drd4UcD5JXmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #smoking history feature angineering\n",
        "def simplify_smoking(status):\n",
        "    if status in ['former', 'ever', 'not current']:\n",
        "        return 'former'\n",
        "    elif status == 'No Info':\n",
        "        return 'unknown'\n",
        "    else:\n",
        "        return status\n",
        "\n",
        "df['smoking_history_simplified'] = df['smoking_history'].apply(simplify_smoking)\n",
        "\n",
        "\n",
        "print(df['smoking_history_simplified'].value_counts())\n",
        "\n",
        "category_counts = df['smoking_history_simplified'].value_counts()\n",
        "category_percentages = category_counts / len(df) * 100\n",
        "\n",
        "\n",
        "print(\"\\nCategory Percentages:\")\n",
        "print(category_percentages)"
      ],
      "metadata": {
        "id": "rtyNBoGgIWQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I calculated the distribution of the simplified smoking_history variable:\n",
        "\n",
        "- The table below shows both the counts and the percentages of each category."
      ],
      "metadata": {
        "id": "-M-KHMCeKKbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='smoking_history_simplified', hue='diabetes', data=df)\n",
        "plt.title('Smoking History (Simplified) by Diabetes')\n",
        "plt.xlabel('Smoking History (Simplified)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_V4pHPHKKTh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "OHAAnBqsK3_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> In this section, I will:\n",
        "- Handle outliers if necessary.\n",
        "- Encode categorical variables.\n",
        "- Scale numerical features.\n",
        "- Split the dataset into training and testing sets for model building.\n",
        "\n"
      ],
      "metadata": {
        "id": "FkkyBJatK-eY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categorical_features = ['gender', 'hypertension', 'heart_disease', 'smoking_history_simplified']\n",
        "\n",
        "\n",
        "df_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n",
        "\n",
        "print(\"Shape after encoding:\", df_encoded.shape)\n"
      ],
      "metadata": {
        "id": "ME2T00DAK7pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "numerical_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "df_encoded[numerical_features] = scaler.fit_transform(df_encoded[numerical_features])\n",
        "\n",
        "df_encoded[numerical_features].head()\n"
      ],
      "metadata": {
        "id": "Hb_tL4sLLUy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X = df_encoded.drop('diabetes', axis=1)\n",
        "y = df_encoded['diabetes']\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n",
        "\n",
        "#I split the data into training and testing sets to evaluate model performance on unseen data and prevent overfitting."
      ],
      "metadata": {
        "id": "OslOZtUKLasx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_encoded = df_encoded.drop(columns=['smoking_history'])\n"
      ],
      "metadata": {
        "id": "_8BWHez_Mm7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jsKNgL2JMvnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Modeling\n",
        "\n",
        "In this section, I will:\n",
        "- Train multiple machine learning models (Logistic Regression, Decision Tree, Random Forest).\n",
        "- Evaluate model performance using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC.\n",
        "- Compare models and select the best one for further analysis.\n"
      ],
      "metadata": {
        "id": "0aLa6dRXMLxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = df_encoded.drop('diabetes', axis=1)\n",
        "y = df_encoded['diabetes']\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "wE-ShERENm1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = lr_model.predict(X_test)\n",
        "y_pred_prob = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, label=f\"ROC-AUC: {roc_auc:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Logistic Regression')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5vHKKs9lN9KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression Results\n",
        "\n",
        "- **Accuracy:** 96% — overall good performance.\n",
        "- **Precision:** 0.88 for diabetic class — good precision in identifying diabetics.\n",
        "- **Recall:** 0.63 for diabetic class — relatively low recall means some diabetics are misclassified as healthy.\n",
        "- **F1-Score:** 0.73 for diabetic class — indicates a trade-off between precision and recall.\n",
        "- **ROC-AUC:** 0.96 — excellent discrimination between classes.\n",
        "\n",
        "- **Recommendation:** Since recall is lower for the diabetic class, further analysis or advanced models may be needed to improve sensitivity.\n"
      ],
      "metadata": {
        "id": "LYZSMFhuOOkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  Decision Tree Classifier\n",
        "\n",
        "In this section, I will train a Decision Tree Classifier to predict diabetes.  \n",
        "Decision Trees can capture nonlinear relationships and handle categorical features well.  \n",
        "We will evaluate its performance using metrics like precision, recall, F1-score, and ROC-AUC.\n"
      ],
      "metadata": {
        "id": "1PF6gVXVOX-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_pred_prob_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "roc_auc_dt = roc_auc_score(y_test, y_pred_prob_dt)\n",
        "print(\"ROC-AUC Score:\", roc_auc_dt)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Decision Tree')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, y_pred_prob_dt)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr_dt, tpr_dt, label=f\"ROC-AUC: {roc_auc_dt:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Decision Tree')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "I3VrCT_5OZ5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree Classifier Results\n",
        "\n",
        "- **Accuracy:** 95% — slightly lower than Logistic Regression (96%).\n",
        "- **Precision (diabetic class):** 0.71 — lower than Logistic Regression (0.88).\n",
        "- **Recall (diabetic class):** 0.72 — higher than Logistic Regression (0.63).\n",
        "- **F1-Score (diabetic class):** 0.72 — similar or slightly better than Logistic Regression.\n",
        "- **ROC-AUC:** 0.85 — lower than Logistic Regression (0.96).\n",
        "\n",
        "- **Observation:** The Decision Tree model detects more diabetic cases (higher recall) but also makes more false positive predictions (lower precision). It may be a better choice if we prioritize finding diabetic cases over minimizing false alarms.\n"
      ],
      "metadata": {
        "id": "9DYOtWPxOuf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier\n",
        "\n",
        "In this section, we will train a Random Forest Classifier to predict diabetes.  \n",
        "Random Forests are ensemble models that can capture complex patterns and often perform better than single trees.  \n",
        "We will evaluate its performance using metrics like precision, recall, F1-score, and ROC-AUC.\n"
      ],
      "metadata": {
        "id": "GNq75Ia_O1yO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_prob_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "roc_auc_rf = roc_auc_score(y_test, y_pred_prob_rf)\n",
        "print(\"ROC-AUC Score:\", roc_auc_rf)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_pred_prob_rf)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr_rf, tpr_rf, label=f\"ROC-AUC: {roc_auc_rf:.2f}\")\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qEJK7yTWO3Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier Results\n",
        "\n",
        "- **Accuracy:** 97% — highest among all models.\n",
        "- **Precision (diabetic class):** 0.95 — highest, indicating very few false positives.\n",
        "- **Recall (diabetic class):** 0.68 — slightly lower than Decision Tree but higher than Logistic Regression.\n",
        "- **F1-Score (diabetic class):** 0.79 — highest overall.\n",
        "- **ROC-AUC:** 0.96 — excellent discrimination between classes.\n",
        "\n",
        "- **Observation:** Random Forest achieves the best balance between precision and recall while maintaining high ROC-AUC. This makes it the best-performing model among the three.\n"
      ],
      "metadata": {
        "id": "RhB4-MdjPGwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance - Random Forest\n",
        "\n",
        "In this section, we will analyze which features the Random Forest Classifier considers most important for predicting diabetes.  \n",
        "This analysis helps us understand which variables contribute most to the model's decision-making process.\n"
      ],
      "metadata": {
        "id": "h1nq469iPQKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "\n",
        "print(feature_importances)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Feature Importance - Random Forest')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b-_sN_ZRPSiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance - Random Forest\n",
        "\n",
        "The most important features for predicting diabetes are:\n",
        "- **HbA1c_level** (0.41) — a strong indicator of blood sugar control.\n",
        "- **blood_glucose_level** (0.33) — directly related to diabetes.\n",
        "- **bmi** (0.12) — an important factor in diabetes risk.\n",
        "- **age** (0.10) — older patients are at higher risk.\n",
        "\n",
        "Other features (hypertension, heart disease, gender, smoking history) contributed less to the model.\n"
      ],
      "metadata": {
        "id": "D5Nt01mLPf27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "\n",
        "In this project, I aimed to build a machine learning model to predict diabetes based on medical and lifestyle features. I started with a thorough exploratory data analysis (EDA), including both numerical and categorical variables, and handled data preprocessing (one-hot encoding, scaling, train/test split).\n",
        "\n",
        "We tested three different machine learning models:\n",
        "- **Logistic Regression:** High ROC-AUC (0.96) but relatively low recall (0.63) for detecting diabetic cases.\n",
        "- **Decision Tree:** Improved recall (0.72) but lower ROC-AUC (0.85), with more false positives.\n",
        "- **Random Forest:** Best overall performance with ROC-AUC (0.96), precision (0.95), and F1-score (0.79) for diabetic class, balancing between precision and recall.\n",
        "\n",
        "The feature importance analysis highlighted:\n",
        "- **HbA1c_level** and **blood_glucose_level** as the most important predictors.\n",
        "- **BMI** and **age** as moderately important.\n",
        "- Other features like **hypertension**, **heart disease**, **gender**, and **smoking history** had minimal impact on the model.\n",
        "\n",
        "**Conclusion:**  \n",
        "- Random Forest is recommended as the best model for predicting diabetes in this dataset.\n",
        "- HbA1c_level and blood_glucose_level are critical features for prediction and should be closely monitored in medical practice.\n",
        "\n",
        "**Next Steps:**  \n",
        "- Further improve recall for diabetic class, possibly with techniques like class weights or SMOTE.\n",
        "- Validate the model on external datasets.\n",
        "- Consider deploying the model in a clinical setting with interpretability tools such as SHAP.\n"
      ],
      "metadata": {
        "id": "tn9sg1WZPqOi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BtGIyTOIPwxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}